import logging
from datetime import datetime
from dateutil.parser import parse
from google.cloud.firestore import Client
from typing import List, Dict
from utils.youtube_utils import normalize_video_item

BATCH_SIZE = 2000
logger = logging.getLogger(__name__)

# Firestore è·¯å¾‘å¸¸æ•¸
def get_batch_doc_ref(db: Client, channel_id: str, batch_index: int):
    return db.collection("channel_data").document(channel_id).collection("videos_batch").document(f"batch_{batch_index}")

def write_batches_to_firestore(db: Client, channel_id: str, new_videos: List[Dict]) -> Dict:
    try:
        # ğŸ” é è™•ç†ï¼šåªä¿ç•™ç‰¹å®šæ¬„ä½
        normalized_videos = []
        for raw in new_videos:
            item = normalize_video_item(raw)
            if not item:
                continue
            video_id = item.get("videoId")
            title = item.get("title")
            publish_date = item.get("publishDate")
            duration = item.get("duration")
            video_type = item.get("type")

            if not all([video_id, title, publish_date, video_type]):
                logger.warning("âš ï¸ ç•¥éä¸å®Œæ•´å½±ç‰‡ï¼š%s", item)
                continue

            normalized_videos.append({
                "videoId": video_id,
                "title": title,
                "publishDate": publish_date,
                "duration": duration,
                "type": video_type
            })

        # âœ… å…ˆå»é™¤è‡ªèº« videoId é‡è¤‡ï¼ˆä¿ç•™æœ€æ–°çš„ï¼‰
        video_map = {}
        for video in normalized_videos:
            video_map[video["videoId"]] = video  # æœƒè¦†è“‹èˆŠçš„
        normalized_videos = list(video_map.values())

        if not normalized_videos:
            logger.info("ğŸ“­ ç„¡æœ‰æ•ˆå½±ç‰‡å¯å¯«å…¥")
            return {
                "batches_written": 0,
                "videos_written": 0
            }

        # å–å¾—ç›®å‰æœ€å¤§çš„ batch index
        batch_col = db.collection("channel_data").document(channel_id).collection("videos_batch")
        docs = list(batch_col.stream())
        batch_indices = [int(doc.id.replace("batch_", "")) for doc in docs if doc.id.startswith("batch_")]
        max_index = max(batch_indices) if batch_indices else -1

        last_index = max_index
        merged_count = 0
        remaining = normalized_videos

        if last_index >= 0:
            last_doc_ref = get_batch_doc_ref(db, channel_id, last_index)
            last_doc = last_doc_ref.get()
            if last_doc.exists:
                data = last_doc.to_dict()
                videos = data.get("videos", [])
                space_left = BATCH_SIZE - len(videos)

                # å»ºç«‹ videoId ç´¢å¼•åŠ é€Ÿæ¯”å°
                existing_map = {v["videoId"]: v for v in videos}
                updated_map = existing_map.copy()

                to_merge = []
                for video in normalized_videos:
                    vid = video["videoId"]
                    if vid in existing_map:
                        updated_map[vid] = video  # âœ… è¦†è“‹åŸæœ‰è³‡æ–™
                        merged_count += 1
                    elif space_left > 0:
                        updated_map[vid] = video
                        to_merge.append(vid)
                        merged_count += 1
                        space_left -= 1

                if merged_count > 0:
                    merged_videos = list(updated_map.values())
                    last_doc_ref.set({"videos": merged_videos})
                    logger.info(f"ğŸ§© åˆä½µ/è¦†è“‹ {merged_count} ç­†åˆ° batch_{last_index}")

                # ç¯©æ‰å·²ç¶“å¯«å…¥/åˆä½µçš„å½±ç‰‡
                written_ids = set(updated_map.keys())
                remaining = [v for v in normalized_videos if v["videoId"] not in written_ids]

        # å‰©ä¸‹çš„è³‡æ–™åˆ†æ‰¹å¯«å…¥
        new_batches = [remaining[i:i + BATCH_SIZE] for i in range(0, len(remaining), BATCH_SIZE)]
        for i, batch in enumerate(new_batches):
            new_index = max_index + 1 + i
            get_batch_doc_ref(db, channel_id, new_index).set({"videos": batch})
            logger.info(f"ğŸ“¦ æ–°å¢ batch_{new_index}ï¼ŒåŒ…å« {len(batch)} ç­†å½±ç‰‡")

        logger.info(f"âœ… å¯«å…¥å®Œæˆï¼Œå…± {len(normalized_videos)} ç­†å½±ç‰‡ï¼Œåˆ†ç‚º {len(new_batches) + (1 if merged_count else 0)} æ‰¹")

        return {
            "batches_written": len(new_batches) + (1 if merged_count else 0),
            "videos_written": len(normalized_videos)
        }

    except Exception as e:
        logger.error("ğŸ”¥ å¯«å…¥ Firestore batch æ™‚ç™¼ç”ŸéŒ¯èª¤: %s", e, exc_info=True)
        return {
            "batches_written": 0,
            "videos_written": 0,
            "error": str(e)
        }
