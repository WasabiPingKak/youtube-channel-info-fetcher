import logging
from datetime import datetime
from dateutil.parser import parse
from google.cloud.firestore import Client
from typing import List, Dict
from utils.youtube_utils import normalize_video_item

BATCH_SIZE = 2000
logger = logging.getLogger(__name__)

# Firestore è·¯å¾‘å¸¸æ•¸
def get_batch_doc_ref(db: Client, channel_id: str, batch_index: int):
    return db.collection("channel_data").document(channel_id).collection("videos_batch").document(f"batch_{batch_index}")

from dateutil.parser import parse

def write_batches_to_firestore(db: Client, channel_id: str, new_videos: List[Dict]) -> Dict:
    try:
        # ğŸ” é è™•ç†ï¼šåªä¿ç•™ç‰¹å®šæ¬„ä½
        normalized_videos = []
        for raw in new_videos:
            item = normalize_video_item(raw)
            if not item:
                continue
            video_id = item.get("videoId")
            title = item.get("title")
            publish_date = item.get("publishDate")
            duration = item.get("duration")
            video_type = item.get("type")

            if not all([video_id, title, publish_date, video_type]):
                logger.warning("âš ï¸ ç•¥éä¸å®Œæ•´å½±ç‰‡ï¼š%s", item)
                continue

            normalized_videos.append({
                "videoId": video_id,
                "title": title,
                "publishDate": publish_date,
                "duration": duration,
                "type": video_type
            })

        if not normalized_videos:
            logger.info("ğŸ“­ ç„¡æœ‰æ•ˆå½±ç‰‡å¯å¯«å…¥")
            return {
                "batches_written": 0,
                "videos_written": 0
            }

        # å–å¾—ç›®å‰æœ€å¤§çš„ batch index
        batch_col = db.collection("channel_data").document(channel_id).collection("videos_batch")
        docs = list(batch_col.stream())
        batch_indices = [int(doc.id.replace("batch_", "")) for doc in docs if doc.id.startswith("batch_")]
        max_index = max(batch_indices) if batch_indices else -1

        last_index = max_index
        merged_count = 0
        if last_index >= 0:
            last_doc_ref = get_batch_doc_ref(db, channel_id, last_index)
            last_doc = last_doc_ref.get()
            if last_doc.exists:
                data = last_doc.to_dict()
                videos = data.get("videos", [])
                space_left = BATCH_SIZE - len(videos)
                if space_left > 0:
                    to_merge = normalized_videos[:space_left]
                    videos.extend(to_merge)
                    last_doc_ref.set({"videos": videos})
                    merged_count = len(to_merge)
                    logger.info(f"ğŸ§© å·²åˆä½µ {merged_count} ç­†åˆ° batch_{last_index}")

        remaining = normalized_videos[merged_count:]
        new_batches = [remaining[i:i + BATCH_SIZE] for i in range(0, len(remaining), BATCH_SIZE)]

        for i, batch in enumerate(new_batches):
            new_index = max_index + 1 + i
            get_batch_doc_ref(db, channel_id, new_index).set({"videos": batch})
            logger.info(f"ğŸ“¦ æ–°å¢ batch_{new_index}ï¼ŒåŒ…å« {len(batch)} ç­†å½±ç‰‡")

        logger.info(f"âœ… å¯«å…¥å®Œæˆï¼Œå…± {len(normalized_videos)} ç­†å½±ç‰‡ï¼Œåˆ†ç‚º {len(new_batches) + (1 if merged_count else 0)} æ‰¹")

        return {
            "batches_written": len(new_batches) + (1 if merged_count else 0),
            "videos_written": len(normalized_videos)
        }

    except Exception as e:
        logger.error("ğŸ”¥ å¯«å…¥ Firestore batch æ™‚ç™¼ç”ŸéŒ¯èª¤: %s", e, exc_info=True)
        return {
            "batches_written": 0,
            "videos_written": 0,
            "error": str(e)
        }