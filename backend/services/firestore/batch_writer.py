import logging
from datetime import datetime
from dateutil.parser import parse
from google.cloud.firestore import Client
from typing import List, Dict
from utils.youtube_utils import normalize_video_item

BATCH_SIZE = 2000
logger = logging.getLogger(__name__)

# Firestore è·¯å¾‘å¸¸æ•¸
def get_batch_doc_ref(db: Client, channel_id: str, batch_index: int):
    return db.collection("channel_data").document(channel_id).collection("videos_batch").document(f"batch_{batch_index}")

from dateutil.parser import parse

def get_last_video_sync_time(db: Client, channel_id: str):
    try:
        index_ref = db.collection("channel_sync_index").document("index_list")
        doc = index_ref.get()
        if not doc.exists:
            return None

        data = doc.to_dict()
        channels = data.get("channels", [])
        for ch in channels:
            if ch.get("channel_id") == channel_id:
                raw_sync = ch.get("lastVideoSyncAt")
                if isinstance(raw_sync, str):
                    return parse(raw_sync)
                elif hasattr(raw_sync, "to_datetime"):
                    return raw_sync.to_datetime()
        return None

    except Exception as e:
        logger.error("ğŸ”¥ ç„¡æ³•è®€å– lastVideoSyncAt (æ–°ç‰ˆ index): %s", e, exc_info=True)
        return None

def write_batches_to_firestore(db: Client, channel_id: str, new_videos: List[Dict]) -> Dict:
    try:
        # ğŸ” é è™•ç†ï¼šåªä¿ç•™ç‰¹å®šæ¬„ä½
        normalized_videos = []
        for raw in new_videos:
            item = normalize_video_item(raw)
            if not item:
                continue
            video_id = item.get("videoId")
            title = item.get("title")
            publish_date = item.get("publishDate")
            duration = item.get("duration")
            video_type = item.get("type")

            if not all([video_id, title, publish_date, video_type]):
                logger.warning("âš ï¸ ç•¥éä¸å®Œæ•´å½±ç‰‡ï¼š%s", item)
                continue

            normalized_videos.append({
                "videoId": video_id,
                "title": title,
                "publishDate": publish_date,
                "duration": duration,
                "type": video_type
            })

        if not normalized_videos:
            logger.info("ğŸ“­ ç„¡æœ‰æ•ˆå½±ç‰‡å¯å¯«å…¥")
            return {
                "batches_written": 0,
                "videos_written": 0
            }

        # å–å¾—ç›®å‰æœ€å¤§çš„ batch index
        batch_col = db.collection("channel_data").document(channel_id).collection("videos_batch")
        docs = list(batch_col.stream())
        batch_indices = [int(doc.id.replace("batch_", "")) for doc in docs if doc.id.startswith("batch_")]
        max_index = max(batch_indices) if batch_indices else -1

        last_index = max_index
        merged_count = 0
        if last_index >= 0:
            last_doc_ref = get_batch_doc_ref(db, channel_id, last_index)
            last_doc = last_doc_ref.get()
            if last_doc.exists:
                data = last_doc.to_dict()
                videos = data.get("videos", [])
                space_left = BATCH_SIZE - len(videos)
                if space_left > 0:
                    to_merge = normalized_videos[:space_left]
                    videos.extend(to_merge)
                    last_doc_ref.set({"videos": videos})
                    merged_count = len(to_merge)
                    logger.info(f"ğŸ§© å·²åˆä½µ {merged_count} ç­†åˆ° batch_{last_index}")

        remaining = normalized_videos[merged_count:]
        new_batches = [remaining[i:i + BATCH_SIZE] for i in range(0, len(remaining), BATCH_SIZE)]

        for i, batch in enumerate(new_batches):
            new_index = max_index + 1 + i
            get_batch_doc_ref(db, channel_id, new_index).set({"videos": batch})
            logger.info(f"ğŸ“¦ æ–°å¢ batch_{new_index}ï¼ŒåŒ…å« {len(batch)} ç­†å½±ç‰‡")

        logger.info(f"âœ… å¯«å…¥å®Œæˆï¼Œå…± {len(normalized_videos)} ç­†å½±ç‰‡ï¼Œåˆ†ç‚º {len(new_batches) + (1 if merged_count else 0)} æ‰¹")

        return {
            "batches_written": len(new_batches) + (1 if merged_count else 0),
            "videos_written": len(normalized_videos)
        }

    except Exception as e:
        logger.error("ğŸ”¥ å¯«å…¥ Firestore batch æ™‚ç™¼ç”ŸéŒ¯èª¤: %s", e, exc_info=True)
        return {
            "batches_written": 0,
            "videos_written": 0,
            "error": str(e)
        }

def update_last_sync_time(db: Client, channel_id: str, new_videos: List[Dict]) -> str:
    if not new_videos:
        logger.info("â„¹ï¸ ç„¡å½±ç‰‡å¯æ›´æ–° lastVideoSyncAt")
        return None

    try:
        latest = max(v["snippet"]["publishedAt"] for v in new_videos)

        index_ref = db.collection("channel_sync_index").document("index_list")
        doc = index_ref.get()

        if not doc.exists:
            # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–æ–°æ¸…å–®
            index_ref.set({
                "channels": [{
                    "channel_id": channel_id,
                    "lastVideoSyncAt": latest
                }]
            })
            logger.info(f"ğŸ•’ [init] å»ºç«‹ index_list ä¸¦åŠ å…¥ {channel_id}")
        else:
            data = doc.to_dict()
            channels = data.get("channels", [])

            found = False
            for c in channels:
                if c.get("channel_id") == channel_id:
                    c["lastVideoSyncAt"] = latest
                    found = True
                    break

            if not found:
                channels.append({
                    "channel_id": channel_id,
                    "lastVideoSyncAt": latest
                })
                logger.info(f"â• [append] æ–°å¢é »é“ {channel_id} è‡³ index_list")

            index_ref.set({"channels": channels})

        logger.info(f"ğŸ•’ æ›´æ–° lastVideoSyncAt ç‚º {latest}")
        return latest

    except Exception as e:
        logger.warning("âš ï¸ ç„¡æ³•æ›´æ–° lastVideoSyncAt: %s", e, exc_info=True)
        return None