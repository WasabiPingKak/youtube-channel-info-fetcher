# /backend/tools/check_batch_video_duplicates.py
# --------------------------------------------------
# CLI å·¥å…·ï¼šæ‰¾å‡º channel_data/*/videos_batch ä¸­é‡è¤‡çš„å½±ç‰‡ ID
# --------------------------------------------------

import os
import sys
import logging
from pathlib import Path
import argparse
from collections import defaultdict

from google.cloud import firestore
from google.api_core.exceptions import GoogleAPIError

# âœ… è¼‰å…¥ Firestore åˆå§‹åŒ–è¨­å®š
sys.path.append(str(Path(__file__).resolve().parents[2]))
from dotenv import load_dotenv
from backend.services.firebase_init_service import init_firestore

load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / ".env.local")
project_root = Path(__file__).resolve().parents[2]
firebase_key_path = (project_root / os.getenv("FIREBASE_KEY_PATH", "")).resolve()
os.environ["FIREBASE_KEY_PATH"] = str(firebase_key_path)
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = str(firebase_key_path)

# âœ… logging è¨­å®š
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")


def fetch_all_channel_ids(db: firestore.Client):
    logging.info("ğŸ“¦ è®€å–æ‰€æœ‰ channel_data/* é »é“ ID ...")
    try:
        docs = db.collection("channel_data").list_documents()
        channel_ids = [doc.id for doc in docs]
        logging.info(f"âœ… å…± {len(channel_ids)} å€‹é »é“")
        return channel_ids
    except GoogleAPIError:
        logging.exception("âŒ ç„¡æ³•åˆ—å‡º channel_data documents")
        return []


def check_channel_batches(db: firestore.Client, channel_id: str, fix: bool = False):
    from collections import defaultdict

    batch_col = db.collection("channel_data").document(channel_id).collection("videos_batch")
    batch_docs = {}
    video_locations = defaultdict(list)  # videoId -> list of (batch_index, batch_id, index_in_batch, video)

    try:
        # è®€å–æ‰€æœ‰ batch æ–‡ä»¶ä¸¦å»ºç«‹åæŸ¥è¡¨
        for doc in batch_col.stream():
            batch_id = doc.id
            if not batch_id.startswith("batch_"):
                continue
            batch_index = int(batch_id.replace("batch_", ""))
            data = doc.to_dict()
            videos = data.get("videos", [])
            batch_docs[batch_id] = videos
            for i, video in enumerate(videos):
                vid = video.get("videoId")
                if vid:
                    video_locations[vid].append((batch_index, batch_id, i, video))

        # æ‰¾å‡ºé‡è¤‡çš„ videoId
        duplicates = {vid: locs for vid, locs in video_locations.items() if len(locs) > 1}
        if not duplicates:
            return  # ç„¡é‡è¤‡å°±ä¸è¼¸å‡º

        print(f"\n====== [channel_id: {channel_id}] é‡è¤‡å½±ç‰‡æª¢æŸ¥å ±å‘Š ======")
        total_videos = sum(len(v) for v in batch_docs.values())
        print(f"âœ… ç¸½å½±ç‰‡æ•¸ï¼š{total_videos}")
        print(f"âš ï¸ ç™¼ç¾é‡è¤‡å½±ç‰‡ï¼š{len(duplicates)} ç­†\n")

        # å»ºç«‹ä¿ç•™è¦å‰‡ï¼švideoId -> (batch_id, index)
        vid_keep_map = {}
        for vid, entries in duplicates.items():
            # æŒ‰ç…§ (batch_index, index_in_batch) æ’åºï¼Œä¿ç•™æœ€å¾Œä¸€ç­†ï¼ˆæœ€å¤§è€…ï¼‰
            entries.sort(key=lambda e: (e[0], e[2]))  # (batch_index, index)
            keep = entries[-1]
            keep_batch_id, keep_index = keep[1], keep[2]
            vid_keep_map[vid] = (keep_batch_id, keep_index)

            print(f"ğŸ” videoId: {vid}")
            print(f"  - å‡ºç¾åœ¨ï¼š{', '.join(e[1] for e in entries)}")
            print(f"  âœ… ä¿ç•™æ–¼ï¼š{keep_batch_id}")

        if fix:
            updated_count = 0
            for batch_id, videos in batch_docs.items():
                updated = []
                for i, v in enumerate(videos):
                    vid = v.get("videoId")
                    if not vid:
                        continue
                    if vid in vid_keep_map:
                        keep_batch_id, keep_index = vid_keep_map[vid]
                        if not (batch_id == keep_batch_id and i == keep_index):
                            continue  # skip, é€™ç­†ä¸æ˜¯ä¿ç•™çš„
                    updated.append(v)

                if len(updated) != len(videos):
                    removed = len(videos) - len(updated)
                    print(f"ğŸ›  ä¿®æ­£ batchï¼š{batch_id}ï¼Œç§»é™¤ {removed} ç­†å½±ç‰‡")
                    batch_col.document(batch_id).set({"videos": updated})
                    updated_count += 1

            if updated_count == 0:
                print("â„¹ï¸ ç„¡éœ€ä¿®æ­£ä»»ä½• batch")

    except Exception as e:
        logging.error(f"âŒ ç„¡æ³•è™•ç†é »é“ {channel_id}ï¼š{e}")


def main():
    parser = argparse.ArgumentParser(description="æ‰¾å‡º channel_data/{channel}/videos_batch ä¸­é‡è¤‡çš„å½±ç‰‡ ID")
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--channel", type=str, help="æŒ‡å®šè¦æª¢æŸ¥çš„é »é“ ID")
    group.add_argument("--all", action="store_true", help="æª¢æŸ¥æ‰€æœ‰é »é“")

    parser.add_argument("--fix", action="store_true", help="ä¿®æ­£é‡è¤‡çš„å½±ç‰‡ï¼Œåªä¿ç•™æœ€æ–°ç‰ˆæœ¬")

    args = parser.parse_args()

    db = init_firestore()

    if args.channel:
        check_channel_batches(db, args.channel, fix=args.fix)
    elif args.all:
        channel_ids = fetch_all_channel_ids(db)
        for cid in channel_ids:
            check_channel_batches(db, cid, fix=args.fix)


if __name__ == "__main__":
    main()
