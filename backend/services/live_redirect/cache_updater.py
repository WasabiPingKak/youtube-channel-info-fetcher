# services/live_redirect/cache_updater.py

import logging
from datetime import timedelta, datetime
from google.cloud.firestore import Client
from services.live_redirect.youtube_api import batch_fetch_video_details
from services.live_redirect.video_classifier import classify_video
from services.live_redirect.fallback_builder import build_fallback_entry
from services.classified_video_fetcher import classify_live_title

logger = logging.getLogger(__name__)


def process_video_ids(db: Client, notify_videos: list[dict], now: datetime) -> dict:
    today_str = now.date().isoformat()
    yesterday_str = (now - timedelta(days=1)).date().isoformat()

    # ğŸ”¹ Step 1ï¼šè¼‰å…¥æ˜¨å¤©èˆ‡ä»Šå¤©çš„å¿«å–
    today_cache = (
        db.collection("live_redirect_cache").document(today_str).get().to_dict() or {}
    )
    yesterday_cache = (
        db.collection("live_redirect_cache").document(yesterday_str).get().to_dict()
        or {}
    )
    raw_old_channels = today_cache.get("channels", []) + yesterday_cache.get(
        "channels", []
    )

    # ğŸ§¹ éæ¿¾å·²æ”¶æ’­è¶…é retention_days çš„èˆŠè³‡æ–™
    retention_days = 3
    old_channels = []
    for c in raw_old_channels:
        end_time = c.get("live", {}).get("endTime")
        if not end_time:
            old_channels.append(c)
        else:
            try:
                end_dt = datetime.fromisoformat(end_time)
                if end_dt >= now - timedelta(days=retention_days):
                    old_channels.append(c)
                else:
                    logging.info(
                        f"ğŸ§¹ æ¸…é™¤éæœŸç›´æ’­ï¼š{c['live'].get('videoId')}ï¼ˆendTime={end_time}ï¼‰"
                    )
            except Exception as e:
                logging.warning(
                    f"âš ï¸ è§£æ endTime å¤±æ•—ï¼š{c['live'].get('videoId')} / {end_time} / error={e}"
                )

    cached_map = {c["live"]["videoId"]: c for c in old_channels}
    end_recorded = {vid for vid, c in cached_map.items() if c["live"].get("endTime")}

    # ğŸ” Step 2ï¼šæ±ºå®šè¦æŸ¥è©¢çš„ videoIds
    notify_ids = {v["videoId"]: v for v in notify_videos if v.get("videoId")}
    logging.info(f"ğŸ“¥ å¾ notify queue æ”¶åˆ°å½±ç‰‡ IDï¼š{list(notify_ids.keys())}")
    query_ids = _filter_video_ids_to_query(notify_ids, cached_map, end_recorded, now)

    logging.info(f"ğŸ“¤ éœ€è¦æŸ¥è©¢ API çš„å½±ç‰‡æ•¸ï¼š{len(query_ids)}")
    logging.info(f"ğŸ“‹ API æŸ¥è©¢å½±ç‰‡IDåˆ—è¡¨ï¼š{query_ids}")

    # ğŸ”¹ Step 3ï¼šæŸ¥è©¢ YouTube API
    yt_items = batch_fetch_video_details(query_ids)
    yt_map = {item["id"]: item for item in yt_items}

    output_channels = []
    processed_ids = set()

    # ğŸ§ª Step 4ï¼šåˆ†é¡å™¨ + fallback
    for video_id in query_ids:
        item = yt_map.get(video_id)

        if item:
            result = classify_video(db, item, now)
            if result:
                channel_id = result.get("channelId") or result.get("channel_id")
                title = result.get("live", {}).get("title", "")

                if not channel_id:
                    logging.warning(f"âš ï¸ ç„¡æ³•åˆ†é¡å½±ç‰‡ {video_id}ï¼šç¼ºå°‘ channelId")
                elif not title:
                    logging.warning(f"âš ï¸ ç„¡æ³•åˆ†é¡å½±ç‰‡ {video_id}ï¼šç¼ºå°‘æ¨™é¡Œ")
                else:
                    category = classify_live_title(db, channel_id, title)
                    result["live"]["category"] = category
                    logging.info(
                        f'ğŸ§© å·²åˆ†é¡ï¼švideoId={video_id}, channelId={channel_id}, title="{title}", '
                        f"category={category.get('matchedCategories')}, pairs={category.get('matchedPairs')}"
                    )

                output_channels.append(result)
                processed_ids.add(video_id)

        else:
            logging.warning(f"âŒ æŸ¥ä¸åˆ°å½±ç‰‡è³‡æ–™ï¼Œfallbackï¼š{video_id}")
            fallback = build_fallback_entry(video_id, now)
            output_channels.append(fallback)
            processed_ids.add(video_id)

    # ğŸ“¦ Step 5ï¼šåˆä½µå¿«å–ï¼Œå·²è™•ç†è€…å„ªå…ˆï¼Œé¿å…é‡è¤‡
    merged_map = {c["live"]["videoId"]: c for c in output_channels}
    for c in old_channels:
        vid = c["live"]["videoId"]
        if vid not in merged_map:
            merged_map[vid] = c
    output_channels = list(merged_map.values())

    # ğŸ“ å›å¯« notify queue çš„ processedAt
    for date_str in [yesterday_str, today_str]:
        ref = db.collection("live_redirect_notify_queue").document(date_str)
        data = ref.get().to_dict() or {}
        new_list = []
        for v in data.get("videos", []):
            if v.get("videoId") in processed_ids:
                v["processedAt"] = now.isoformat()
            new_list.append(v)
        ref.set({"updatedAt": now.isoformat(), "videos": new_list})

    # ğŸ•’ æ‡¶æ›´æ–°æ©Ÿåˆ¶ï¼šè£œæŸ¥å¿«å–ä¸­æœªæ”¶æ’­çš„å½±ç‰‡
    lazy_result = _lazy_refresh_endtime(db, old_channels, cached_map, end_recorded, now)
    lazy_channels = lazy_result["channels"]
    processed_ids.update([c["live"]["videoId"] for c in lazy_channels])

    # åˆä½µåŸæœ¬èˆ‡æ‡¶æ›´æ–°çš„å¿«å–çµæœï¼ˆä»¥æ‡¶æ›´æ–°ç‚ºä¸»ï¼‰
    output_channels = list(
        {c["live"]["videoId"]: c for c in (output_channels + lazy_channels)}.values()
    )

    # ğŸ¯ å¾Œè£œåˆ†é¡ï¼šé‡å°å°šæœªåˆ†é¡çš„å½±ç‰‡è£œä¸Š live.category
    for channel in output_channels:
        live = channel.get("live", {})
        if "category" not in live:
            channel_id = channel.get("channelId") or channel.get("channel_id")
            title = live.get("title", "")
            if channel_id and title:
                category = classify_live_title(db, channel_id, title)
                live["category"] = category
                logging.info(
                    f"ğŸ“Œ å¾Œè£œåˆ†é¡ï¼švideoId={live.get('videoId')}, channelId={channel_id}, "
                    f"category={category.get('matchedCategories')}, pairs={category.get('matchedPairs')}"
                )

    db.collection("live_redirect_cache").document(today_str).set(
        {"updatedAt": now.isoformat(), "channels": output_channels}
    )

    return {"updatedAt": now.isoformat(), "channels": output_channels}


def _filter_video_ids_to_query(
    notify_ids: dict[str, dict],
    cached_map: dict[str, dict],
    end_recorded: set[str],
    now: datetime,
) -> list[str]:
    """
    æ±ºå®šéœ€è¦é€åˆ° YouTube API æŸ¥è©¢çš„ videoId æ¸…å–®ã€‚

    éæ¿¾æ¢ä»¶ï¼š
    - æ’é™¤å·²æ”¶æ’­å½±ç‰‡ï¼ˆendTime å­˜åœ¨ï¼‰
    - æ’é™¤å¿«å–ä¸­ã€Œé ç´„æ™‚é–“æ™šæ–¼ now + 15 åˆ†é˜ã€çš„ç›´æ’­

    Args:
        notify_ids: ä¾†è‡ª notify queue çš„ videoId å°æ‡‰ dict
        cached_map: å¿«å–ä¸­çš„å½±ç‰‡è³‡æ–™ (videoId â†’ channel dict)
        end_recorded: å·²æ”¶æ’­çš„ videoId é›†åˆ
        now: ç•¶å‰æ™‚é–“

    Returns:
        list[str]: éœ€è¦æŸ¥è©¢çš„ videoIds
    """
    # ğŸ” é¡å¤–åˆ—å‡º cached_map ä¸­ endTime ç‚º null çš„å½±ç‰‡
    no_endtime_ids = [
        vid for vid, c in cached_map.items() if not c.get("live", {}).get("endTime")
    ]
    logging.info(f"ğŸ”„ å¿«å–ä¸­å°šæœªæ”¶æ’­çš„å½±ç‰‡ IDï¼š{no_endtime_ids}")

    result = []

    for vid, v in notify_ids.items():
        if vid in end_recorded:
            logging.info(f"âœ… å·²æ”¶æ’­å½±ç‰‡ç•¥éï¼š{vid}")
            continue

        cache = cached_map.get(vid)
        if cache:
            live = cache.get("live", {})
            scheduled = live.get("startTime")
            is_upcoming = live.get("isUpcoming")

            if is_upcoming and scheduled:
                try:
                    start_time = datetime.fromisoformat(scheduled)
                    if start_time > now + timedelta(minutes=15):
                        # é ç´„å½±ç‰‡æ™‚é–“è¶…é 15 åˆ†é˜
                        continue
                except Exception as e:
                    logging.warning(
                        f"âš ï¸ è§£æ startTime å¤±æ•—ï¼š{vid} / {scheduled} / error={e}"
                    )
        else:
            logging.info(f"ğŸ†• å…¨æ–°å½±ç‰‡ï¼Œç„¡å¿«å–ç´€éŒ„ï¼š{vid}")

        result.append(vid)

    logging.info(f"âœ… æœ€çµ‚é€å‡ºæŸ¥è©¢çš„å½±ç‰‡ IDï¼š{result}")
    return result


def _lazy_refresh_endtime(
    db: Client,
    old_channels: list[dict],
    cached_map: dict[str, dict],
    end_recorded: set[str],
    now: datetime,
) -> dict:
    """
    å˜—è©¦è£œæŸ¥å¿«å–ä¸­å°šæœªæ”¶æ’­çš„å½±ç‰‡ï¼Œè£œä¸Š endTimeã€‚

    Args:
        db: Firestore è³‡æ–™åº«
        old_channels: ç¾æœ‰å¿«å–ä¸­çš„æ‰€æœ‰å½±ç‰‡è³‡æ–™
        cached_map: å¿«å–å½±ç‰‡ mapï¼ˆvideoId â†’ channelï¼‰
        end_recorded: å·²æ”¶æ’­çš„å½±ç‰‡ ID é›†åˆ
        now: ç•¶å‰æ™‚é–“

    Returns:
        dict: {"channels": [...]} è£œå®Œå¾Œçš„å½±ç‰‡åˆ—è¡¨
    """
    # æ‰¾å‡ºå¿«å–ä¸­ endTime ç‚º null çš„å½±ç‰‡
    pending_ids = [
        c["live"]["videoId"] for c in old_channels if not c["live"].get("endTime")
    ]

    logging.info(f"ğŸ•’ æ‡¶æ›´æ–°ï¼šç™¼ç¾å¿«å–ä¸­å°šæœªæ”¶æ’­å½±ç‰‡ {len(pending_ids)} æ”¯ï¼Œæº–å‚™æŸ¥è©¢")

    # ä½¿ç”¨èˆ‡ä¸»æµç¨‹ç›¸åŒçš„æŸ¥è©¢é‚è¼¯èˆ‡åˆ†é¡å™¨
    notify_items = {vid: {"videoId": vid} for vid in pending_ids}
    query_ids = _filter_video_ids_to_query(notify_items, cached_map, end_recorded, now)

    yt_items = batch_fetch_video_details(query_ids)
    yt_map = {item["id"]: item for item in yt_items}

    refreshed_channels = []

    for video_id in query_ids:
        item = yt_map.get(video_id)
        if item:
            result = classify_video(db, item, now)
            if result:
                refreshed_channels.append(result)
        else:
            logging.warning(f"âŒ æ‡¶æ›´æ–°ï¼šæŸ¥ç„¡å½±ç‰‡è³‡æ–™ fallbackï¼š{video_id}")
            fallback = build_fallback_entry(video_id, now)
            refreshed_channels.append(fallback)

    video_ids = [c["live"]["videoId"] for c in refreshed_channels]
    logging.info(
        f"âœ… æ‡¶æ›´æ–°å®Œæˆï¼Œå…±å¯«å…¥ {len(refreshed_channels)} ç­†ï¼ŒvideoIds={video_ids}"
    )

    return {"channels": refreshed_channels}
