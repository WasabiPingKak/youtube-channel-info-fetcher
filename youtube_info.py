import os
import csv
import json
import argparse
import datetime
import pytz
import isodate
import googleapiclient.discovery
import googleapiclient.errors

from config import API_KEY, INPUT_CHANNEL


# ğŸ¥ CLI åƒæ•¸è™•ç†
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--output', choices=['console', 'csv', 'json'], default='console',
                        help='è¼¸å‡ºæ–¹å¼ï¼šconsoleï¼ˆé è¨­ï¼‰ã€csvã€json')
    return parser.parse_args()


# ğŸ¥ è¨­å®š API æœå‹™
def get_youtube_service():
    return googleapiclient.discovery.build("youtube", "v3", developerKey=API_KEY)


# ğŸ¥ æ ¹æ“š @å¸³è™Ÿ æˆ– channelId å‚³å›æ­£ç¢ºçš„ channelId
def get_channel_id(youtube, input_channel):
    if input_channel.startswith("UC"):
        return input_channel
    # Will increase API query, NOT recommend
    if input_channel.startswith("@"):
        username = input_channel[1:]
    else:
        username = input_channel

    try:
        response = youtube.search().list(part="snippet", q=username, type="channel", maxResults=1).execute()
        if response["items"]:
            return response["items"][0]["snippet"]["channelId"]
        else:
            print("âŒ æ‰¾ä¸åˆ°é »é“")
            return None
    except googleapiclient.errors.HttpError as err:
        print(f"HTTP Error: {err}")
        return None


# ğŸ¥ å–å¾—é »é“ä¸Šå‚³å½±ç‰‡æ¸…å–® ID
def get_uploads_playlist_id(youtube, channel_id):
    try:
        response = youtube.channels().list(part="contentDetails", id=channel_id).execute()
        return response['items'][0]['contentDetails']['relatedPlaylists']['uploads']
    except Exception as e:
        print(f"Error getting uploads playlist: {e}")
        return None


# ğŸ¥ å–å¾—æ’­æ”¾æ¸…å–®ä¸­æ‰€æœ‰å½±ç‰‡ ID
def get_video_ids_from_playlist(youtube, playlist_id):
    video_ids = []
    next_page_token = None
    while True:
        request = youtube.playlistItems().list(
            part='contentDetails',
            playlistId=playlist_id,
            maxResults=50,
            pageToken=next_page_token
        )
        response = request.execute()
        video_ids += [item['contentDetails']['videoId'] for item in response['items']]
        next_page_token = response.get('nextPageToken')
        if not next_page_token:
            break
    return video_ids


# ğŸ¥ æ‰¹æ¬¡å–å¾—å½±ç‰‡è©³ç´°è³‡æ–™
def fetch_video_details(youtube, video_ids):
    video_details = []
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i + 50]
        try:
            response = youtube.videos().list(
                part='snippet,contentDetails,liveStreamingDetails',
                id=','.join(batch)
            ).execute()
            video_details.extend(response['items'])
        except Exception as e:
            print(f"Error fetching video details: {e}")
    return video_details


# ğŸ¥ è½‰æ›å½±ç‰‡é•·åº¦ç‚º HH:MM:SS ä¸¦å–å¾—ç¸½åˆ†é˜æ•¸
def convert_duration_to_hms(duration):
    parsed = isodate.parse_duration(duration)
    total_seconds = int(parsed.total_seconds())
    hours = total_seconds // 3600
    minutes = (total_seconds % 3600) // 60
    seconds = total_seconds % 60
    total_minutes = (hours * 60) + minutes + (seconds / 60)
    total_minutes = round(total_minutes, 1) if total_minutes < 1 else int(total_minutes)
    return f"{hours:02}:{minutes:02}:{seconds:02}", total_minutes


# ğŸ¥ å–å¾—ç™¼å¸ƒæ—¥æœŸèˆ‡æ˜ŸæœŸ
def get_video_publish_date(video):
    dt = datetime.datetime.fromisoformat(video['snippet']['publishedAt'][:-1])
    local_dt = dt.astimezone(pytz.timezone("Asia/Taipei"))
    #weekdays = ['ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'å…­', 'æ—¥']
    #return local_dt.strftime(f"%Y/%m/%d ({weekdays[local_dt.weekday()]})")
    return local_dt.strftime(f"%Y/%m/%d")


# ğŸ¥ åˆ¤æ–·å½±ç‰‡é¡å‹ï¼šå½±ç‰‡ã€Shorts æˆ–ç›´æ’­æª”
def get_video_type(video):
    if 'liveStreamingDetails' in video and 'actualEndTime' in video['liveStreamingDetails']:
        return "ç›´æ’­æª”"
    if video['snippet'].get('liveBroadcastContent') == 'upcoming':
        return None
    duration_minutes = convert_duration_to_hms(video['contentDetails']['duration'])[1]
    if duration_minutes <= 1:
        return "Shorts"
    return "å½±ç‰‡"


# ğŸ¥ è®€å– video_date_ranges.conf
def load_date_ranges():
    if not os.path.exists('video_date_ranges.conf'):
        return None

    with open('video_date_ranges.conf', encoding='utf-8') as f:
        lines = [line.strip() for line in f if line.strip()]

    if len(lines) != 2:
        print("âŒ video_date_ranges.conf æª”æ¡ˆæ ¼å¼éŒ¯èª¤ï¼Œæ‡‰åŒ…å«é–‹å§‹èˆ‡çµæŸå…©å€‹æ—¥æœŸ")
        return None

    try:
        start_date = datetime.datetime.strptime(lines[0], "%Y/%m/%d")
        end_date = datetime.datetime.strptime(lines[1], "%Y/%m/%d")
        tz = pytz.timezone("Asia/Taipei")
        return [(tz.localize(start_date), tz.localize(end_date))]
    except Exception as e:
        print(f"âŒ æ—¥æœŸæ ¼å¼éŒ¯èª¤ï¼Œè«‹ä½¿ç”¨ YYYY/MM/DD æ ¼å¼ï¼š{e}")
        return None


# ğŸ¥ ä¸»ç¨‹å¼
def main():
    args = parse_args()
    youtube = get_youtube_service()
    channel_id = get_channel_id(youtube, INPUT_CHANNEL)
    playlist_id = get_uploads_playlist_id(youtube, channel_id)
    video_ids = get_video_ids_from_playlist(youtube, playlist_id)
    all_videos = fetch_video_details(youtube, video_ids)
    date_ranges = load_date_ranges()

    results = []
    for video in all_videos:
        video_type = get_video_type(video)
        if not video_type:
            continue

        published_dt = datetime.datetime.fromisoformat(video['snippet']['publishedAt'][:-1]).astimezone(pytz.timezone("Asia/Taipei"))
        if date_ranges and not any(start <= published_dt < end for start, end in date_ranges):
            continue

        duration_text, total_minutes = convert_duration_to_hms(video['contentDetails']['duration'])
        title = video['snippet']['title']
        video_id = video['id']
        date_text = get_video_publish_date(video)

        category = None
        if "ã€" in title and "ã€‘" in title:
            category = title.split("ã€")[1].split("ã€‘")[0]

        results.append({
            "æ¨™é¡Œ": title,
            "å½±ç‰‡ID": video_id,
            "ç™¼å¸ƒæ—¥æœŸ": date_text,
            "å½±ç‰‡æ™‚é•·": duration_text,
            "ç¸½åˆ†é˜æ•¸": total_minutes,
            "é¡åˆ¥": category or "ç„¡",
            "å½±ç‰‡é¡å‹": video_type
        })

    # è¼¸å‡ºçµæœ
    if args.output == 'console':
        for r in results:
            print(f"æ¨™é¡Œ: {r['æ¨™é¡Œ']}")
            print(f"å½±ç‰‡ID: {r['å½±ç‰‡ID']}")
            print(f"ç™¼å¸ƒæ—¥æœŸ: {r['ç™¼å¸ƒæ—¥æœŸ']}")
            print(f"å½±ç‰‡æ™‚é•·: {r['å½±ç‰‡æ™‚é•·']}")
            print(f"ç¸½åˆ†é˜æ•¸: {r['ç¸½åˆ†é˜æ•¸']}")
            print(f"é¡åˆ¥: {r['é¡åˆ¥']}")
            print(f"å½±ç‰‡é¡å‹: {r['å½±ç‰‡é¡å‹']}")
            print("-" * 40)

    elif args.output == 'csv':
        with open("output.csv", "w", encoding='utf-8-sig', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=results[0].keys())
            writer.writeheader()
            writer.writerows(results)
        print("âœ… å·²è¼¸å‡ºç‚º output.csv")

    elif args.output == 'json':
        with open("output.json", "w", encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print("âœ… å·²è¼¸å‡ºç‚º output.json")


if __name__ == "__main__":
    main()
