import logging
from datetime import datetime, timezone
from google.cloud.firestore import Client
from services.firestore.channel_loader import load_all_channels_from_index_list
from services.heatmap.utils import convert_matrix_to_count
from services.heatmap.metadata_loader import build_channel_metadata_lookup

def build_weekly_heatmap_cache(db: Client):
    # é å…ˆè®€å–åŸºæœ¬è³‡æ–™ mapping
    metadata_lookup = build_channel_metadata_lookup(db)
    channels = load_all_channels_from_index_list(db)

    result = []
    missing_count = 0
    skipped_without_meta = 0

    for channel in channels:
        channel_id = channel.get("channel_id")
        if not channel_id:
            continue

        meta = metadata_lookup.get(channel_id)
        if not meta:
            logging.warning(f"âš ï¸ {channel_id} ç„¡å°æ‡‰çš„ metadataï¼Œè·³é")
            skipped_without_meta += 1
            continue

        doc_ref = db.document(f"channel_data/{channel_id}/heat_map/channel_video_heatmap")
        doc = doc_ref.get()
        if not doc.exists:
            logging.warning(f"âš ï¸ {channel_id} heatmap æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³é")
            missing_count += 1
            continue

        data = doc.to_dict()
        all_range = data.get("all_range")
        if not all_range:
            logging.warning(f"âš ï¸ {channel_id} æœªåŒ…å« all_rangeï¼Œè·³é")
            missing_count += 1
            continue

        matrix = all_range.get("matrix", {})
        active_time, total_count = convert_matrix_to_count(matrix)

        result.append({
            "channelId": channel_id,
            "name": meta.get("name"),
            "thumbnail": meta.get("thumbnail"),
            "countryCode": meta.get("countryCode"),
            "activeTime": active_time,
            "totalCount": total_count,
        })

    logging.info(f"ğŸ“¦ å¿«å–å»ºç«‹å®Œæˆï¼šå…± {len(result)} ç­†é »é“ï¼Œç•¥é heatmap ç¼ºå¤± {missing_count} ç­†ï¼Œmeta ç¼ºå¤± {skipped_without_meta} ç­†")
    return {
        "version": 1,
        "generatedAt": datetime.now(timezone.utc).isoformat(),
        "channels": result
    }

def write_weekly_heatmap_cache(db: Client):
    try:
        # Step 1: ä¸»é«”è³‡æ–™ä¾†è‡ª build_weekly_heatmap_cache()
        weekly_data = build_weekly_heatmap_cache(db)
        weekly_channels = weekly_data.get("channels", [])

        # Step 2: è®€å– pending è³‡æ–™ï¼ˆè‹¥å­˜åœ¨ï¼‰
        pending_ref = db.collection("stats_cache").document("active_time_pending")
        pending_doc = pending_ref.get()
        pending_data = pending_doc.to_dict() if pending_doc.exists else {}
        pending_channels = pending_data.get("channels", [])

        logging.info(f"ğŸ”„ è®€å– pending å¿«å–ï¼š{len(pending_channels)} ç­†")

        # Step 3: åˆä½µä¸¦å»é‡ï¼ˆchannelId ç‚º keyï¼‰
        combined = {c["channelId"]: c for c in weekly_channels}
        for p in pending_channels:
            combined[p["channelId"]] = p  # ç”¨ pending è¦†è“‹åŒ ID

        merged_channels = list(combined.values())
        logging.info(f"ğŸ“ åˆä½µå¾Œé »é“ç¸½æ•¸ï¼š{len(merged_channels)}")

        # Step 4: å¯«å…¥ merged çµæœ
        ref = db.collection("stats_cache").document("active_time_weekly")
        weekly_data["channels"] = merged_channels
        ref.set(weekly_data)
        logging.info(f"âœ… å·²è¦†è“‹å¯«å…¥ active_time_weeklyï¼Œå…± {len(merged_channels)} ç­†")

        # Step 5: æ¸…ç©º pending
        pending_ref.set({"channels": []})
        logging.info("ğŸ§¹ å·²æ¸…ç©º active_time_pending")

        return True
    except Exception as e:
        logging.error(f"ğŸ”¥ å¯«å…¥ weekly heatmap cache å¤±æ•—ï¼š{e}")
        return False

def append_to_pending_cache(db, channel_id: str):
    """
    å°‡å–®ä¸€æ–°åˆå§‹åŒ–é »é“çš„æ´»èº heatmap çµ±è¨ˆçµæœå¯«å…¥ pending å¿«å–æ–‡ä»¶ï¼ˆé¿å…é‡è¤‡ï¼‰

    ä¾†æºï¼š
    - activeTime: å¾ Firestore çš„ all_range.matrix çµ±è¨ˆ count
    - metadata: å¾ channel_index_batch è£¡æŸ¥ name / thumbnail / countryCode
    """
    try:
        # ğŸ” Step 1: è®€å– heatmap matrix
        doc_ref = db.document(f"channel_data/{channel_id}/heat_map/channel_video_heatmap")
        doc = doc_ref.get()
        if not doc.exists:
            logging.warning(f"âš ï¸ [pending] {channel_id} heatmap æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç„¡æ³•åŠ å…¥å¿«å–")
            return

        all_range = doc.to_dict().get("all_range")
        if not all_range:
            logging.warning(f"âš ï¸ [pending] {channel_id} ç„¡ all_rangeï¼Œç„¡æ³•åŠ å…¥å¿«å–")
            return

        matrix = all_range.get("matrix", {})
        active_time, total_count = convert_matrix_to_count(matrix)

        # ğŸ” Step 2: æŸ¥æ‰¾ metadata
        metadata_lookup = build_channel_metadata_lookup(db)
        meta = metadata_lookup.get(channel_id)
        if not meta:
            logging.warning(f"âš ï¸ [pending] æ‰¾ä¸åˆ° {channel_id} çš„ metadataï¼Œç•¥é")
            return

        # ğŸ”ƒ Step 3: è®€å–ç¾æœ‰ pending é™£åˆ—
        pending_ref = db.collection("stats_cache").document("active_time_pending")
        pending_doc = pending_ref.get()
        pending_data = pending_doc.to_dict() if pending_doc.exists else {}
        current_channels = pending_data.get("channels", [])

        # å»ºç«‹æ–°è³‡æ–™
        new_entry = {
            "channelId": channel_id,
            "name": meta.get("name"),
            "thumbnail": meta.get("thumbnail"),
            "countryCode": meta.get("countryCode"),
            "activeTime": active_time,
            "totalCount": total_count
        }

        # éæ¿¾èˆŠè³‡æ–™ï¼ˆåŒ channelIdï¼‰
        filtered = [c for c in current_channels if c.get("channelId") != channel_id]
        filtered.append(new_entry)

        # å¯«å…¥å› Firestore
        pending_ref.set({"channels": filtered})
        logging.info(f"ğŸŸ£ [pending] å·²å°‡ {channel_id} åŠ å…¥ active_time_pending å¿«å–ï¼ˆå…± {len(filtered)} ç­†ï¼‰")

    except Exception as e:
        logging.error(f"ğŸ”¥ å¯«å…¥ active_time_pending å¿«å–å¤±æ•—ï¼š{e}")
