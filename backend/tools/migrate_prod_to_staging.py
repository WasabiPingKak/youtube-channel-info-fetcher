#!/usr/bin/env python3
"""
migrate_prod_to_staging.py
--------------------------
å°‡ Production Firestore è³‡æ–™åº«è¤‡è£½åˆ° Staging ç’°å¢ƒ

åŠŸèƒ½ï¼š
- å®Œæ•´è¤‡è£½æ‰€æœ‰ Collections èˆ‡ Subcollections
- æ”¯æ´è³‡æ–™éæ¿¾ï¼ˆä¿ç•™ N å¤©è³‡æ–™ï¼‰
- è‡ªå‹•è„«æ•ï¼ˆç§»é™¤ OAuth tokensï¼‰
- å®‰å…¨æª¢æŸ¥ï¼ˆé˜²æ­¢åå‘è¤‡è£½ï¼‰
- é€²åº¦é¡¯ç¤ºèˆ‡çµ±è¨ˆ

ä½¿ç”¨ç¯„ä¾‹ï¼š
  python tools/migrate_prod_to_staging.py --full --days 90
  python tools/migrate_prod_to_staging.py --full --dry-run
  python tools/migrate_prod_to_staging.py --collections channel_data,channel_index_batch
"""

import sys
import os
import argparse
import logging
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Optional, Set
from collections import defaultdict

# åŠ å…¥å°ˆæ¡ˆæ ¹ç›®éŒ„ä»¥åŒ¯å…¥æ¨¡çµ„
sys.path.append(str(Path(__file__).resolve().parents[2]))

from dotenv import load_dotenv
from google.cloud import firestore
from google.oauth2 import service_account
from google.api_core.exceptions import GoogleAPIError

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / ".env.local")
project_root = Path(__file__).resolve().parents[2]
firebase_key_path = (project_root / os.getenv("FIREBASE_KEY_PATH", "")).resolve()

os.environ["FIREBASE_KEY_PATH"] = str(firebase_key_path)
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = str(firebase_key_path)

# Logging è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)


# ============================================================================
# å¸¸æ•¸å®šç¾©
# ============================================================================

# æ‰€æœ‰ Collection åˆ—è¡¨ï¼ˆæŒ‰è¤‡è£½é †åºï¼‰
ALL_COLLECTIONS = [
    "global_settings",              # å…¨åŸŸè¨­å®šï¼ˆå„ªå…ˆï¼‰
    "channel_index_batch",          # é »é“ç´¢å¼•æ‰¹æ¬¡
    "channel_data",                 # é »é“è³‡æ–™ï¼ˆå« subcollectionsï¼‰
    "channel_sync_index",           # åŒæ­¥è¿½è¹¤
    "trending_games_daily",         # è¶¨å‹¢è³‡æ–™
    "stats_cache",                  # å¿«å–è³‡æ–™
    "live_redirect_cache",          # ç›´æ’­å¿«å–
    "live_redirect_notify_queue",   # é€šçŸ¥ä½‡åˆ—
    "channel_index",                # éºç•™ç´¢å¼•ï¼ˆæœ€å¾Œï¼‰
]

# éœ€è¦è„«æ•çš„è·¯å¾‘ï¼ˆç§»é™¤æ•æ„Ÿè³‡æ–™ï¼‰
SANITIZE_PATHS = [
    "channel_data/*/channel_info/meta",  # OAuth tokens
]

# éœ€è¦éè¿´è¤‡è£½ subcollections çš„ collections
RECURSIVE_COLLECTIONS = {
    "channel_data",  # éœ€è¦è¤‡è£½æ‰€æœ‰ subcollections
}

# Batch write å¤§å°é™åˆ¶
BATCH_SIZE = 500

# ============================================================================
# Firestore å®¢æˆ¶ç«¯åˆå§‹åŒ–
# ============================================================================

def init_firestore_clients():
    """
    åˆå§‹åŒ– Production å’Œ Staging çš„ Firestore å®¢æˆ¶ç«¯

    Returns:
        tuple: (source_db, target_db, source_db_id, target_db_id)
    """
    # æ˜ç¢ºæŒ‡å®šè³‡æ–™åº« ID
    source_db_id = os.getenv("SOURCE_FIRESTORE_DATABASE", "(default)")
    target_db_id = os.getenv("TARGET_FIRESTORE_DATABASE", "staging")

    logger.info(f"åˆå§‹åŒ– Firestore å®¢æˆ¶ç«¯...")
    logger.info(f"  ä¾†æºè³‡æ–™åº«: {source_db_id}")
    logger.info(f"  ç›®æ¨™è³‡æ–™åº«: {target_db_id}")

    try:
        credentials = service_account.Credentials.from_service_account_file(
            str(firebase_key_path)
        )
        project_id = credentials.project_id

        source_db = firestore.Client(
            credentials=credentials,
            project=project_id,
            database=source_db_id
        )

        target_db = firestore.Client(
            credentials=credentials,
            project=project_id,
            database=target_db_id
        )

        logger.info("âœ“ Firestore å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸ")
        return source_db, target_db, source_db_id, target_db_id

    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ– Firestore å®¢æˆ¶ç«¯å¤±æ•—: {e}")
        raise


# ============================================================================
# å®‰å…¨æª¢æŸ¥
# ============================================================================

def validate_migration_direction(source_db_id: str, target_db_id: str):
    """
    é©—è­‰é·ç§»æ–¹å‘ï¼Œé˜²æ­¢èª¤æ“ä½œ
    """
    logger.info("åŸ·è¡Œå®‰å…¨æª¢æŸ¥...")

    # æª¢æŸ¥ï¼šç¦æ­¢å¾ Staging è¤‡è£½åˆ° Production
    if source_db_id == "staging" and target_db_id == "(default)":
        logger.error("âŒ å®‰å…¨æª¢æŸ¥å¤±æ•—ï¼")
        logger.error("âŒ ç¦æ­¢å¾ Staging è¤‡è£½åˆ° Productionï¼")
        logger.error("âŒ é€™å€‹æ“ä½œæœƒè¦†è“‹æ­£å¼ç’°å¢ƒè³‡æ–™ï¼Œè«‹æª¢æŸ¥ç’°å¢ƒè®Šæ•¸è¨­å®šã€‚")
        sys.exit(1)

    # æª¢æŸ¥ï¼šä¾†æºå’Œç›®æ¨™ä¸èƒ½ç›¸åŒ
    if source_db_id == target_db_id:
        logger.error("âŒ å®‰å…¨æª¢æŸ¥å¤±æ•—ï¼")
        logger.error(f"âŒ ä¾†æºå’Œç›®æ¨™è³‡æ–™åº«ç›¸åŒ ({source_db_id})ï¼")
        logger.error("âŒ è«‹æª¢æŸ¥ç’°å¢ƒè®Šæ•¸è¨­å®šã€‚")
        sys.exit(1)

    logger.info("âœ“ å®‰å…¨æª¢æŸ¥é€šé")
    logger.info(f"  è¤‡è£½æ–¹å‘: {source_db_id} â†’ {target_db_id}")


def confirm_migration(source_db_id: str, target_db_id: str, dry_run: bool):
    """
    è¦æ±‚ä½¿ç”¨è€…ç¢ºèªé·ç§»æ“ä½œ
    """
    if dry_run:
        logger.info("ğŸš§ Dry Run æ¨¡å¼ï¼šä¸æœƒå¯¦éš›å¯«å…¥è³‡æ–™")
        return

    print("\n" + "=" * 60)
    print("âš ï¸  è­¦å‘Šï¼šæ­¤æ“ä½œå°‡è¦†è“‹ Staging è³‡æ–™åº«çš„æ‰€æœ‰è³‡æ–™ï¼")
    print("=" * 60)
    print(f"ä¾†æºè³‡æ–™åº«: {source_db_id}")
    print(f"ç›®æ¨™è³‡æ–™åº«: {target_db_id}")
    print("=" * 60)

    try:
        response = input("\nè«‹è¼¸å…¥ 'yes' ä»¥ç¢ºèªç¹¼çºŒ: ").strip().lower()
        if response != 'yes':
            print("âŒ æ“ä½œå·²å–æ¶ˆ")
            sys.exit(0)
    except KeyboardInterrupt:
        print("\nâŒ æ“ä½œå·²å–æ¶ˆ")
        sys.exit(0)

    print("\né–‹å§‹åŸ·è¡Œé·ç§»...\n")


# ============================================================================
# è³‡æ–™è„«æ•
# ============================================================================

def should_sanitize_document(doc_path: str, sanitize: bool) -> bool:
    """
    åˆ¤æ–·æ–‡ä»¶æ˜¯å¦éœ€è¦è„«æ•

    Args:
        doc_path: æ–‡ä»¶è·¯å¾‘ï¼Œä¾‹å¦‚ "channel_data/UCxxx/channel_info/meta"
        sanitize: æ˜¯å¦å•Ÿç”¨è„«æ•æ¨¡å¼

    Returns:
        bool: æ˜¯å¦éœ€è¦è„«æ•
    """
    if not sanitize:
        return False

    for pattern in SANITIZE_PATHS:
        # ç°¡å–®çš„æ¨¡å¼åŒ¹é…ï¼ˆæ”¯æ´ * è¬ç”¨å­—å…ƒï¼‰
        pattern_parts = pattern.split("/")
        path_parts = doc_path.split("/")

        if len(pattern_parts) != len(path_parts):
            continue

        match = all(
            p == "*" or p == path_part
            for p, path_part in zip(pattern_parts, path_parts)
        )

        if match:
            return True

    return False


def sanitize_document_data(data: Dict, doc_path: str) -> Dict:
    """
    ç§»é™¤æ–‡ä»¶ä¸­çš„æ•æ„Ÿè³‡æ–™

    Args:
        data: åŸå§‹æ–‡ä»¶è³‡æ–™
        doc_path: æ–‡ä»¶è·¯å¾‘

    Returns:
        Dict: è„«æ•å¾Œçš„è³‡æ–™
    """
    sanitized = data.copy()

    # ç§»é™¤ OAuth tokens
    sensitive_fields = ["refresh_token", "access_token"]
    removed_fields = []

    for field in sensitive_fields:
        if field in sanitized:
            del sanitized[field]
            removed_fields.append(field)

    if removed_fields:
        logger.debug(f"  è„«æ•: {doc_path} (ç§»é™¤ {', '.join(removed_fields)})")

    return sanitized


# ============================================================================
# æ–‡ä»¶éæ¿¾
# ============================================================================

def should_copy_document(
    doc_id: str,
    data: Dict,
    collection_name: str,
    days_filter: Optional[int]
) -> bool:
    """
    åˆ¤æ–·æ–‡ä»¶æ˜¯å¦æ‡‰è©²è¢«è¤‡è£½

    Args:
        doc_id: æ–‡ä»¶ ID
        data: æ–‡ä»¶è³‡æ–™
        collection_name: Collection åç¨±
        days_filter: ä¿ç•™å¤©æ•¸ï¼ˆNone è¡¨ç¤ºå…¨éƒ¨è¤‡è£½ï¼‰

    Returns:
        bool: æ˜¯å¦æ‡‰è©²è¤‡è£½
    """
    # ç„¡éæ¿¾æ¢ä»¶ï¼Œå…¨éƒ¨è¤‡è£½
    if days_filter is None:
        return True

    # æ™‚æ•ˆæ€§è³‡æ–™ï¼šæ ¹æ“šæ–‡ä»¶ ID åˆ¤æ–·ï¼ˆYYYY-MM-DD æ ¼å¼ï¼‰
    if collection_name in ["trending_games_daily", "live_redirect_cache", "live_redirect_notify_queue"]:
        try:
            # æ–‡ä»¶ ID æ ¼å¼: YYYY-MM-DD
            doc_date = datetime.strptime(doc_id, "%Y-%m-%d").replace(tzinfo=timezone.utc)
            cutoff_date = datetime.now(timezone.utc) - timedelta(days=days_filter)
            return doc_date >= cutoff_date
        except ValueError:
            # ç„¡æ³•è§£ææ—¥æœŸï¼Œå…¨éƒ¨è¤‡è£½
            return True

    # å…¶ä»– Collectionï¼šæ ¹æ“š updatedAt æˆ– createdAt æ¬„ä½
    date_fields = ["updatedAt", "createdAt", "joinedAt", "publishDate"]
    for field in date_fields:
        if field in data:
            try:
                field_value = data[field]

                # è™•ç† Firestore Timestamp
                if hasattr(field_value, "timestamp"):
                    doc_timestamp = field_value
                    cutoff_date = datetime.now(timezone.utc) - timedelta(days=days_filter)
                    return datetime.fromtimestamp(
                        doc_timestamp.timestamp(), tz=timezone.utc
                    ) >= cutoff_date

                # è™•ç† ISO 8601 å­—ä¸²
                elif isinstance(field_value, str):
                    doc_date = datetime.fromisoformat(
                        field_value.replace("Z", "+00:00")
                    )
                    cutoff_date = datetime.now(timezone.utc) - timedelta(days=days_filter)
                    return doc_date >= cutoff_date

            except (ValueError, AttributeError):
                continue

    # æ²’æœ‰æ™‚é–“æ¬„ä½ï¼Œå…¨éƒ¨è¤‡è£½
    return True


# ============================================================================
# Collection è¤‡è£½
# ============================================================================

def copy_collection(
    source_db: firestore.Client,
    target_db: firestore.Client,
    collection_name: str,
    sanitize: bool = True,
    days_filter: Optional[int] = None,
    dry_run: bool = False,
    stats: Dict = None
) -> int:
    """
    è¤‡è£½å–®å€‹ Collection

    Returns:
        int: è¤‡è£½çš„æ–‡ä»¶æ•¸é‡
    """
    logger.info(f"[{collection_name}] é–‹å§‹è¤‡è£½...")

    try:
        source_col = source_db.collection(collection_name)
        target_col = target_db.collection(collection_name)

        # è®€å–æ‰€æœ‰æ–‡ä»¶
        docs = list(source_col.stream())
        total_docs = len(docs)
        copied_docs = 0
        skipped_docs = 0

        if total_docs == 0:
            logger.info(f"[{collection_name}] ç„¡æ–‡ä»¶ï¼Œç•¥é")
            return 0

        logger.info(f"[{collection_name}] æ‰¾åˆ° {total_docs} å€‹æ–‡ä»¶")

        # æ‰¹æ¬¡å¯«å…¥
        batch = target_db.batch()
        batch_count = 0

        for i, doc in enumerate(docs, 1):
            doc_id = doc.id
            data = doc.to_dict()

            if data is None:
                continue

            # éæ¿¾ï¼šæª¢æŸ¥æ˜¯å¦æ‡‰è©²è¤‡è£½
            if not should_copy_document(doc_id, data, collection_name, days_filter):
                skipped_docs += 1
                continue

            # è„«æ•ï¼šç§»é™¤æ•æ„Ÿè³‡æ–™
            doc_path = f"{collection_name}/{doc_id}"
            if should_sanitize_document(doc_path, sanitize):
                data = sanitize_document_data(data, doc_path)

            # å¯«å…¥ç›®æ¨™è³‡æ–™åº«
            if not dry_run:
                target_doc_ref = target_col.document(doc_id)
                batch.set(target_doc_ref, data)
                batch_count += 1

                # æ¯ BATCH_SIZE ç­†æäº¤ä¸€æ¬¡
                if batch_count >= BATCH_SIZE:
                    batch.commit()
                    batch = target_db.batch()
                    batch_count = 0

            copied_docs += 1

            # é€²åº¦é¡¯ç¤º
            if i % 100 == 0 or i == total_docs:
                progress = (i / total_docs) * 100
                logger.info(
                    f"[{collection_name}] é€²åº¦: {i}/{total_docs} "
                    f"({progress:.1f}%) - è¤‡è£½ {copied_docs} ç­†"
                )

        # æäº¤å‰©é¤˜çš„ batch
        if batch_count > 0 and not dry_run:
            batch.commit()

        logger.info(
            f"[{collection_name}] å®Œæˆ - "
            f"è¤‡è£½ {copied_docs} ç­†ï¼Œç•¥é {skipped_docs} ç­†"
        )

        # æ›´æ–°çµ±è¨ˆ
        if stats is not None:
            stats["total_docs"] += copied_docs
            stats["skipped_docs"] += skipped_docs

        # éè¿´è¤‡è£½ subcollections
        if collection_name in RECURSIVE_COLLECTIONS:
            subcol_count = copy_subcollections(
                source_db, target_db, collection_name, docs,
                sanitize, days_filter, dry_run, stats
            )
            logger.info(
                f"[{collection_name}] Subcollections è¤‡è£½å®Œæˆ - {subcol_count} å€‹æ–‡ä»¶"
            )

        return copied_docs

    except GoogleAPIError as e:
        logger.error(f"[{collection_name}] Firestore å­˜å–éŒ¯èª¤: {e}")
        raise
    except Exception as e:
        logger.error(f"[{collection_name}] è¤‡è£½å¤±æ•—: {e}")
        raise


def copy_subcollections(
    source_db: firestore.Client,
    target_db: firestore.Client,
    parent_collection: str,
    parent_docs: List,
    sanitize: bool,
    days_filter: Optional[int],
    dry_run: bool,
    stats: Dict
) -> int:
    """
    éè¿´è¤‡è£½ Subcollections

    Returns:
        int: è¤‡è£½çš„æ–‡ä»¶ç¸½æ•¸
    """
    total_copied = 0

    for parent_doc in parent_docs:
        parent_id = parent_doc.id

        # å–å¾—æ‰€æœ‰ subcollections
        subcols = parent_doc.reference.collections()

        for subcol in subcols:
            subcol_name = subcol.id
            subcol_path = f"{parent_collection}/{parent_id}/{subcol_name}"

            # è®€å– subcollection çš„æ‰€æœ‰æ–‡ä»¶
            subdocs = list(subcol.stream())

            if not subdocs:
                continue

            logger.info(f"  [{subcol_path}] æ‰¾åˆ° {len(subdocs)} å€‹æ–‡ä»¶")

            # æ‰¹æ¬¡å¯«å…¥
            batch = target_db.batch()
            batch_count = 0
            copied_count = 0

            for subdoc in subdocs:
                subdoc_id = subdoc.id
                subdata = subdoc.to_dict()

                if subdata is None:
                    continue

                # éæ¿¾
                if not should_copy_document(subdoc_id, subdata, subcol_name, days_filter):
                    continue

                # è„«æ•
                subdoc_path = f"{subcol_path}/{subdoc_id}"
                if should_sanitize_document(subdoc_path, sanitize):
                    subdata = sanitize_document_data(subdata, subdoc_path)

                # å¯«å…¥
                if not dry_run:
                    target_parent_ref = target_db.collection(parent_collection).document(parent_id)
                    target_subdoc_ref = target_parent_ref.collection(subcol_name).document(subdoc_id)
                    batch.set(target_subdoc_ref, subdata)
                    batch_count += 1

                    if batch_count >= BATCH_SIZE:
                        batch.commit()
                        batch = target_db.batch()
                        batch_count = 0

                copied_count += 1

            # æäº¤å‰©é¤˜çš„ batch
            if batch_count > 0 and not dry_run:
                batch.commit()

            logger.info(f"  [{subcol_path}] å®Œæˆ - è¤‡è£½ {copied_count} ç­†")
            total_copied += copied_count

            if stats is not None:
                stats["total_docs"] += copied_count

    return total_copied


# ============================================================================
# ä¸»ç¨‹å¼
# ============================================================================

def parse_arguments():
    """è§£æå‘½ä»¤åˆ—åƒæ•¸"""
    parser = argparse.ArgumentParser(
        description="å°‡ Production Firestore è³‡æ–™åº«è¤‡è£½åˆ° Staging ç’°å¢ƒ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¯„ä¾‹:
  # å®Œæ•´è¤‡è£½ï¼ˆä¿ç•™ 90 å¤©è³‡æ–™ï¼Œè‡ªå‹•è„«æ•ï¼‰
  python tools/migrate_prod_to_staging.py --full --days 90

  # å®Œæ•´è¤‡è£½æ‰€æœ‰æ­·å²è³‡æ–™
  python tools/migrate_prod_to_staging.py --full --all-history

  # åªè¤‡è£½æŒ‡å®š Collections
  python tools/migrate_prod_to_staging.py --collections channel_data,channel_index_batch --days 90

  # Dry Run æ¨¡å¼ï¼ˆä¸å¯¦éš›å¯«å…¥ï¼‰
  python tools/migrate_prod_to_staging.py --full --days 90 --dry-run

  # ä¸è„«æ•æ¨¡å¼ï¼ˆä¿ç•™ OAuth tokensï¼Œåƒ…ç”¨æ–¼ç‰¹æ®Šæ¸¬è©¦ï¼‰
  python tools/migrate_prod_to_staging.py --full --days 90 --no-sanitize
        """
    )

    # æ¨¡å¼é¸æ“‡
    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument(
        "--full",
        action="store_true",
        help="å®Œæ•´è¤‡è£½æ‰€æœ‰ Collections"
    )
    mode_group.add_argument(
        "--collections",
        type=str,
        help="åªè¤‡è£½æŒ‡å®šçš„ Collectionsï¼ˆé€—è™Ÿåˆ†éš”ï¼‰ï¼Œä¾‹å¦‚: channel_data,channel_index_batch"
    )

    # è³‡æ–™éæ¿¾
    filter_group = parser.add_mutually_exclusive_group()
    filter_group.add_argument(
        "--days",
        type=int,
        default=90,
        help="ä¿ç•™æœ€è¿‘ N å¤©çš„è³‡æ–™ï¼ˆé è¨­: 90ï¼‰"
    )
    filter_group.add_argument(
        "--all-history",
        action="store_true",
        help="è¤‡è£½æ‰€æœ‰æ­·å²è³‡æ–™ï¼ˆä¸éæ¿¾ï¼‰"
    )

    # è„«æ•é¸é …
    parser.add_argument(
        "--no-sanitize",
        action="store_true",
        help="ä¸è„«æ•ï¼ˆä¿ç•™ OAuth tokens ç­‰æ•æ„Ÿè³‡æ–™ï¼‰"
    )

    # Dry Run
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Dry Run æ¨¡å¼ï¼šåªé¡¯ç¤ºæœƒè¤‡è£½ä»€éº¼ï¼Œä¸å¯¦éš›åŸ·è¡Œ"
    )

    return parser.parse_args()


def main():
    """ä¸»ç¨‹å¼é€²å…¥é»"""
    args = parse_arguments()

    # åˆå§‹åŒ–
    source_db, target_db, source_db_id, target_db_id = init_firestore_clients()

    # å®‰å…¨æª¢æŸ¥
    validate_migration_direction(source_db_id, target_db_id)
    confirm_migration(source_db_id, target_db_id, args.dry_run)

    # æ±ºå®šè¦è¤‡è£½çš„ Collections
    if args.full:
        collections_to_copy = ALL_COLLECTIONS
    else:
        collections_to_copy = [c.strip() for c in args.collections.split(",")]

    # è³‡æ–™éæ¿¾è¨­å®š
    days_filter = None if args.all_history else args.days
    sanitize = not args.no_sanitize

    # çµ±è¨ˆè³‡è¨Š
    stats = {
        "total_collections": len(collections_to_copy),
        "total_docs": 0,
        "skipped_docs": 0,
        "start_time": datetime.now(),
    }

    # é¡¯ç¤ºé…ç½®
    print("=" * 70)
    print("Firestore Migration Tool")
    print(f"{source_db_id} â†’ {target_db_id}")
    print("=" * 70)
    print(f"Collections: {', '.join(collections_to_copy)}")
    print(f"è³‡æ–™éæ¿¾: {'æ‰€æœ‰æ­·å²è³‡æ–™' if days_filter is None else f'æœ€è¿‘ {days_filter} å¤©'}")
    print(f"è„«æ•æ¨¡å¼: {'æ˜¯' if sanitize else 'å¦'}")
    print(f"Dry Run: {'æ˜¯' if args.dry_run else 'å¦'}")
    print("=" * 70)
    print()

    # åŸ·è¡Œè¤‡è£½
    try:
        for i, collection_name in enumerate(collections_to_copy, 1):
            print(f"\n[{i}/{len(collections_to_copy)}] æ­£åœ¨è¤‡è£½: {collection_name}")
            print("-" * 70)

            copy_collection(
                source_db=source_db,
                target_db=target_db,
                collection_name=collection_name,
                sanitize=sanitize,
                days_filter=days_filter,
                dry_run=args.dry_run,
                stats=stats
            )

        # é¡¯ç¤ºæ‘˜è¦
        stats["end_time"] = datetime.now()
        stats["duration"] = stats["end_time"] - stats["start_time"]

        print("\n" + "=" * 70)
        print("Migration Summary")
        print("=" * 70)
        print(f"Total Collections: {stats['total_collections']}")
        print(f"Total Documents Copied: {stats['total_docs']:,}")
        print(f"Documents Skipped: {stats['skipped_docs']:,}")
        print(f"Total Time: {stats['duration']}")
        print(f"Status: {'âœ“ Dry Run Completed' if args.dry_run else 'âœ“ Success'}")
        print("=" * 70)

    except KeyboardInterrupt:
        print("\n\nâŒ æ“ä½œè¢«ä½¿ç”¨è€…ä¸­æ–·")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâŒ é·ç§»éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
